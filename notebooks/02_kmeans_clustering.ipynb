import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

# Set style for better plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("ðŸš— Vehicle Clustering Analysis")
print("=" * 50)

# 1. DATA GENERATION
def generate_vehicle_data(n_samples=300, random_state=42):
    """Generate synthetic vehicle dataset"""
    np.random.seed(random_state)
    
    data = {
        'Weight': np.random.randint(1000, 3000, n_samples),
        'EngineSize': np.random.uniform(1.0, 4.0, n_samples),
        'Horsepower': np.random.randint(50, 300, n_samples)
    }
    
    return pd.DataFrame(data)

# Generate dataset
df = generate_vehicle_data(n_samples=300)
print(f"Dataset shape: {df.shape}")
print("\nFirst 5 rows:")
print(df.head())

print("\nDataset statistics:")
print(df.describe())

# 2. EXPLORATORY DATA ANALYSIS
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Distribution plots
df['Weight'].hist(bins=30, ax=axes[0,0], alpha=0.7, color='skyblue')
axes[0,0].set_title('Weight Distribution')
axes[0,0].set_xlabel('Weight (lbs)')

df['EngineSize'].hist(bins=30, ax=axes[0,1], alpha=0.7, color='lightgreen')
axes[0,1].set_title('Engine Size Distribution')
axes[0,1].set_xlabel('Engine Size (L)')

df['Horsepower'].hist(bins=30, ax=axes[1,0], alpha=0.7, color='salmon')
axes[1,0].set_title('Horsepower Distribution')
axes[1,0].set_xlabel('Horsepower')

# Correlation heatmap
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1,1])
axes[1,1].set_title('Feature Correlation Matrix')

plt.tight_layout()
plt.show()

# Pairplot for feature relationships
plt.figure(figsize=(12, 8))
pd.plotting.scatter_matrix(df, alpha=0.6, figsize=(12, 8), diagonal='hist')
plt.suptitle('Vehicle Features Pairplot', y=0.95)
plt.tight_layout()
plt.show()

# 3. DATA PREPROCESSING
# Standardize features for better clustering
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)
X_scaled_df = pd.DataFrame(X_scaled, columns=df.columns)

print("Standardized data statistics:")
print(X_scaled_df.describe())

# 4. ELBOW METHOD FOR OPTIMAL K
inertias = []
silhouette_scores = []
k_range = range(2, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))

# Plot elbow curve and silhouette scores
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)
ax1.set_xlabel('Number of Clusters (k)')
ax1.set_ylabel('Inertia (Within-cluster sum of squares)')
ax1.set_title('Elbow Method for Optimal k')
ax1.grid(True, alpha=0.3)

ax2.plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)
ax2.set_xlabel('Number of Clusters (k)')
ax2.set_ylabel('Silhouette Score')
ax2.set_title('Silhouette Analysis')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Find optimal k
optimal_k_elbow = k_range[np.argmax(np.diff(np.diff(inertias)) > 0)] + 2
optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]

print(f"Suggested k (Elbow method): {optimal_k_elbow}")
print(f"Suggested k (Silhouette): {optimal_k_silhouette}")

# 5. PERFORM CLUSTERING WITH OPTIMAL K
optimal_k = 3  # Based on analysis
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(X_scaled)

# Add cluster labels to original dataframe
df['Cluster'] = cluster_labels

print(f"\nClustering completed with k={optimal_k}")
print(f"Silhouette Score: {silhouette_score(X_scaled, cluster_labels):.3f}")

# 6. CLUSTER ANALYSIS
print("\nCluster Statistics:")
print("=" * 30)

cluster_stats = df.groupby('Cluster').agg({
    'Weight': ['mean', 'std', 'count'],
    'EngineSize': ['mean', 'std'],
    'Horsepower': ['mean', 'std']
}).round(2)

print(cluster_stats)

# 7. VISUALIZATION
# Create comprehensive visualization
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Scatter plots for different feature combinations
feature_pairs = [('Weight', 'Horsepower'), ('Weight', 'EngineSize'), ('EngineSize', 'Horsepower')]
colors = ['red', 'blue', 'green', 'orange', 'purple']

for i, (x_feature, y_feature) in enumerate(feature_pairs):
    ax = axes[0, i]
    
    for cluster in range(optimal_k):
        cluster_data = df[df['Cluster'] == cluster]
        ax.scatter(cluster_data[x_feature], cluster_data[y_feature], 
                  c=colors[cluster], label=f'Cluster {cluster}', alpha=0.7, s=50)
    
    # Plot centroids
    centroids_original = scaler.inverse_transform(kmeans.cluster_centers_)
    centroids_df = pd.DataFrame(centroids_original, columns=df.columns[:-1])
    
    ax.scatter(centroids_df[x_feature], centroids_df[y_feature], 
              c='black', marker='x', s=200, linewidths=3, label='Centroids')
    
    ax.set_xlabel(x_feature)
    ax.set_ylabel(y_feature)
    ax.set_title(f'{x_feature} vs {y_feature}')
    ax.legend()
    ax.grid(True, alpha=0.3)

# Cluster size distribution
axes[1, 0].pie(df['Cluster'].value_counts().sort_index(), 
               labels=[f'Cluster {i}' for i in range(optimal_k)],
               colors=colors[:optimal_k], autopct='%1.1f%%', startangle=90)
axes[1, 0].set_title('Cluster Size Distribution')

# Feature importance (centroid values)
centroids_df_plot = pd.DataFrame(centroids_original, 
                                columns=['Weight', 'EngineSize', 'Horsepower'],
                                index=[f'Cluster {i}' for i in range(optimal_k)])

centroids_df_plot.plot(kind='bar', ax=axes[1, 1], color=colors[:3])
axes[1, 1].set_title('Cluster Centroids (Original Scale)')
axes[1, 1].set_ylabel('Feature Values')
axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
axes[1, 1].tick_params(axis='x', rotation=45)

# Box plots for each feature by cluster
df_melted = df.melt(id_vars=['Cluster'], 
                    value_vars=['Weight', 'EngineSize', 'Horsepower'],
                    var_name='Feature', value_name='Value')

sns.boxplot(data=df_melted, x='Feature', y='Value', hue='Cluster', ax=axes[1, 2])
axes[1, 2].set_title('Feature Distribution by Cluster')
axes[1, 2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# 8. CLUSTER INTERPRETATION
print("\nðŸŽ¯ Cluster Interpretation:")
print("=" * 40)

for cluster in range(optimal_k):
    cluster_data = df[df['Cluster'] == cluster]
    avg_weight = cluster_data['Weight'].mean()
    avg_engine = cluster_data['EngineSize'].mean()
    avg_hp = cluster_data['Horsepower'].mean()
    count = len(cluster_data)
    
    print(f"\nCluster {cluster} ({count} vehicles):")
    print(f"  Average Weight: {avg_weight:.0f} lbs")
    print(f"  Average Engine Size: {avg_engine:.1f} L")
    print(f"  Average Horsepower: {avg_hp:.0f} HP")
    
    # Interpret cluster characteristics
    if avg_weight < 1700 and avg_hp < 150:
        category = "Economy/Compact Vehicles"
    elif avg_weight > 2300 or avg_hp > 200:
        category = "Performance/Large Vehicles"
    else:
        category = "Mid-range Vehicles"
    
    print(f"  Category: {category}")

# 9. SAVE RESULTS
# Save clustered data
df.to_csv('../data/processed/clustered_data.csv', index=False)
print(f"\nðŸ’¾ Results saved to 'data/processed/clustered_data.csv'")

# Export cluster centers
centroids_export = pd.DataFrame(centroids_original, 
                               columns=['Weight', 'EngineSize', 'Horsepower'])
centroids_export.to_csv('../data/processed/cluster_centers.csv', index=False)
print("ðŸ“Š Cluster centers saved to 'data/processed/cluster_centers.csv'")

print("\nâœ… Analysis complete! Check the web app for interactive exploration.")
