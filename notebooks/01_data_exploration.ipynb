{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚗 Vehicle Clustering Analysis - Data Exploration\n",
    "\n",
    "This notebook provides comprehensive exploratory data analysis (EDA) for vehicle clustering analysis.\n",
    "\n",
    "## 📋 Table of Contents\n",
    "1. [Data Loading & Overview](#data-loading)\n",
    "2. [Descriptive Statistics](#descriptive-stats)\n",
    "3. [Data Quality Assessment](#data-quality)\n",
    "4. [Feature Distribution Analysis](#distributions)\n",
    "5. [Correlation Analysis](#correlations)\n",
    "6. [Outlier Detection](#outliers)\n",
    "7. [Feature Engineering Opportunities](#feature-engineering)\n",
    "8. [Data Preprocessing for Clustering](#preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"📚 Libraries loaded successfully!\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Overview {#data-loading}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or generate synthetic vehicle data\n",
    "def generate_vehicle_data(n_samples=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic vehicle dataset with realistic features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of vehicles to generate\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Vehicle dataset\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Vehicle categories with different characteristics\n",
    "    categories = {\n",
    "        'Economy': {\n",
    "            'mpg': (25, 40),\n",
    "            'horsepower': (120, 200),\n",
    "            'price': (15000, 30000),\n",
    "            'weight': (2500, 3300),\n",
    "            'engine_size': (1.0, 2.5)\n",
    "        },\n",
    "        'Performance': {\n",
    "            'mpg': (15, 25),\n",
    "            'horsepower': (250, 450),\n",
    "            'price': (35000, 85000),\n",
    "            'weight': (3200, 4400),\n",
    "            'engine_size': (2.5, 6.0)\n",
    "        },\n",
    "        'Utility': {\n",
    "            'mpg': (12, 20),\n",
    "            'horsepower': (200, 350),\n",
    "            'price': (25000, 60000),\n",
    "            'weight': (4000, 6000),\n",
    "            'engine_size': (3.0, 8.0)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    vehicle_types = ['Sedan', 'SUV', 'Truck', 'Compact', 'Luxury', 'Coupe']\n",
    "    fuel_types = ['Gasoline', 'Hybrid', 'Electric', 'Diesel']\n",
    "    brands = ['Toyota', 'Ford', 'Chevrolet', 'Honda', 'BMW', 'Mercedes', 'Audi', 'Volkswagen']\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Select category with some probability\n",
    "        category = np.random.choice(list(categories.keys()), p=[0.4, 0.3, 0.3])\n",
    "        cat_specs = categories[category]\n",
    "        \n",
    "        # Generate features based on category\n",
    "        vehicle = {\n",
    "            'vehicle_id': f'V{i+1:04d}',\n",
    "            'category': category,\n",
    "            'brand': np.random.choice(brands),\n",
    "            'vehicle_type': np.random.choice(vehicle_types),\n",
    "            'fuel_type': np.random.choice(fuel_types),\n",
    "            'mpg': np.random.uniform(*cat_specs['mpg']),\n",
    "            'horsepower': np.random.uniform(*cat_specs['horsepower']),\n",
    "            'price': np.random.uniform(*cat_specs['price']),\n",
    "            'weight': np.random.uniform(*cat_specs['weight']),\n",
    "            'engine_size': np.random.uniform(*cat_specs['engine_size']),\n",
    "            'year': np.random.randint(2015, 2024),\n",
    "            'mileage': np.random.randint(0, 150000)\n",
    "        }\n",
    "        \n",
    "        # Add some noise and correlations\n",
    "        if vehicle['fuel_type'] == 'Electric':\n",
    "            vehicle['mpg'] = np.random.uniform(90, 130)  # MPGe for electric\n",
    "            vehicle['engine_size'] = 0.0\n",
    "        elif vehicle['fuel_type'] == 'Hybrid':\n",
    "            vehicle['mpg'] *= 1.3  # Hybrids get better mileage\n",
    "            \n",
    "        data.append(vehicle)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Round numerical values\n",
    "    df['mpg'] = df['mpg'].round(1)\n",
    "    df['horsepower'] = df['horsepower'].round(0).astype(int)\n",
    "    df['price'] = df['price'].round(0).astype(int)\n",
    "    df['weight'] = df['weight'].round(0).astype(int)\n",
    "    df['engine_size'] = df['engine_size'].round(1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_vehicle_data(n_samples=1000)\n",
    "print(f\"✅ Dataset generated with {len(df)} vehicles\")\n",
    "print(f\"📏 Dataset shape: {df.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n🔍 First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"📋 Dataset Information:\")\n",
    "print(\"=\" * 50)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n📊 Dataset Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"• Total vehicles: {len(df):,}\")\n",
    "print(f\"• Features: {len(df.columns)}\")\n",
    "print(f\"• Numerical features: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"• Categorical features: {len(df.select_dtypes(include=['object']).columns)}\")\n",
    "print(f\"• Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descriptive Statistics {#descriptive-stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features statistics\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'vehicle_id' in numerical_cols:\n",
    "    numerical_cols.remove('vehicle_id')\n",
    "\n",
    "print(\"🔢 Numerical Features Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "df[numerical_cols].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features analysis\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'vehicle_id' in categorical_cols:\n",
    "    categorical_cols.remove('vehicle_id')\n",
    "\n",
    "print(\"📝 Categorical Features Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    value_counts = df[col].value_counts()\n",
    "    percentages = (value_counts / len(df) * 100).round(1)\n",
    "    \n",
    "    for value, count in value_counts.items():\n",
    "        pct = percentages[value]\n",
    "        print(f\"  • {value}: {count} ({pct}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment {#data-quality}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print(\"❓ Missing Values Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) == 0:\n",
    "    print(\"✅ No missing values found!\")\n",
    "else:\n",
    "    print(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate analysis\n",
    "print(\"🔄 Duplicate Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "total_duplicates = df.duplicated().sum()\n",
    "duplicate_percent = (total_duplicates / len(df) * 100).round(2)\n",
    "\n",
    "print(f\"• Total duplicate rows: {total_duplicates} ({duplicate_percent}%)\")\n",
    "\n",
    "# Check for duplicates excluding vehicle_id\n",
    "cols_except_id = [col for col in df.columns if col != 'vehicle_id']\n",
    "content_duplicates = df.duplicated(subset=cols_except_id).sum()\n",
    "content_duplicate_percent = (content_duplicates / len(df) * 100).round(2)\n",
    "\n",
    "print(f\"• Content duplicate rows (excluding ID): {content_duplicates} ({content_duplicate_percent}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type consistency check\n",
    "print(\"🏷️ Data Type Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for col in df.columns:\n",
    "    dtype = df[col].dtype\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"• {col}: {dtype} (unique values: {unique_count})\")\n",
    "    \n",
    "    # Check for potential type issues\n",
    "    if dtype == 'object' and col not in ['vehicle_id', 'category', 'brand', 'vehicle_type', 'fuel_type']:\n",
    "        print(f\"  ⚠️ Warning: {col} is object type but might be numerical\")\n",
    "    \n",
    "    if dtype in ['int64', 'float64'] and unique_count < 10:\n",
    "        print(f\"  ℹ️ Info: {col} has few unique values, might be categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Distribution Analysis {#distributions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for numerical features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('📊 Distribution of Numerical Features', fontsize=16, fontweight='bold')\n",
    "\n",
    "numerical_features = ['mpg', 'horsepower', 'price', 'weight', 'engine_size', 'year']\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    row, col = i // 3, i % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    sns.histplot(data=df, x=feature, kde=True, ax=ax, alpha=0.7)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = df[feature].mean()\n",
    "    median_val = df[feature].median()\n",
    "    \n",
    "    ax.axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Mean: {mean_val:.1f}')\n",
    "    ax.axvline(median_val, color='green', linestyle='--', alpha=0.8, label=f'Median: {median_val:.1f}')\n",
    "    \n",
    "    ax.set_title(f'{feature.replace(\"_\", \" \").title()} Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Skewness and kurtosis analysis\n",
    "print(\"\\n📐 Distribution Shape Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Feature':<15} {'Skewness':<10} {'Kurtosis':<10} {'Normality Test':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for feature in numerical_features:\n",
    "    skewness = stats.skew(df[feature])\n",
    "    kurtosis_val = stats.kurtosis(df[feature])\n",
    "    \n",
    "    # Shapiro-Wilk test for normality (sample if dataset is large)\n",
    "    if len(df) > 5000:\n",
    "        sample_data = df[feature].sample(5000, random_state=42)\n",
    "    else:\n",
    "        sample_data = df[feature]\n",
    "    \n",
    "    _, p_value = stats.shapiro(sample_data)\n",
    "    is_normal = \"Normal\" if p_value > 0.05 else \"Non-normal\"\n",
    "    \n",
    "    print(f\"{feature:<15} {skewness:<10.3f} {kurtosis_val:<10.3f} {is_normal:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('📊 Categorical Features Distribution', fontsize=16, fontweight='bold')\n",
    "\n",
    "categorical_features = ['category', 'fuel_type', 'vehicle_type', 'brand']\n",
    "\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    row, col = i // 2, i % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Count plot\n",
    "    value_counts = df[feature].value_counts()\n",
    "    \n",
    "    if len(value_counts) > 8:  # If too many categories, show top 8\n",
    "        value_counts = value_counts.head(8)\n",
    "        title_suffix = \" (Top 8)\"\n",
    "    else:\n",
    "        title_suffix = \"\"\n",
    "    \n",
    "    bars = ax.bar(range(len(value_counts)), value_counts.values)\n",
    "    ax.set_xticks(range(len(value_counts)))\n",
    "    ax.set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "    ax.set_title(f'{feature.replace(\"_\", \" \").title()} Distribution{title_suffix}')\n",
    "    ax.set_ylabel('Count')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, value_counts.values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                str(value), ha='center', va='bottom')\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis {#correlations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "correlation_features = ['mpg', 'horsepower', 'price', 'weight', 'engine_size', 'year', 'mileage']\n",
    "corr_matrix = df[correlation_features].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(corr_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.3f',\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "\n",
    "plt.title('🔗 Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify strong correlations\n",
    "print(\"\\n🔍 Strong Correlations (|r| > 0.5):\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "strong_correlations = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:\n",
    "            feature1 = corr_matrix.columns[i]\n",
    "            feature2 = corr_matrix.columns[j]\n",
    "            strong_correlations.append((feature1, feature2, corr_val))\n",
    "\n",
    "if strong_correlations:\n",
    "    for feat1, feat2, corr_val in sorted(strong_correlations, key=lambda x: abs(x[2]), reverse=True):\n",
    "        direction = \"positive\" if corr_val > 0 else \"negative\"\n",
    "        print(f\"• {feat1} ↔ {feat2}: {corr_val:.3f} ({direction})\")\n",
    "else:\n",
    "    print(\"No strong correlations found (|r| > 0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for key relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('🔗 Key Feature Relationships', fontsize=16, fontweight='bold')\n",
    "\n",
    "relationships = [\n",
    "    ('horsepower', 'mpg', 'Horsepower vs MPG'),\n",
    "    ('weight', 'mpg', 'Weight vs MPG'),\n",
    "    ('price', 'horsepower', 'Price vs Horsepower'),\n",
    "    ('engine_size', 'horsepower', 'Engine Size vs Horsepower')\n",
    "]\n",
    "\n",
    "for i, (x_feature, y_feature, title) in enumerate(relationships):\n",
    "    row, col = i // 2, i % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Scatter plot with trend line\n",
    "    sns.scatterplot(data=df, x=x_feature, y=y_feature, \n",
    "                   hue='category', alpha=0.7, ax=ax)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(df[x_feature], df[y_feature], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(df[x_feature], p(df[x_feature]), \"r--\", alpha=0.8, linewidth=2)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr_val = df[x_feature].corr(df[y_feature])\n",
    "    ax.text(0.05, 0.95, f'r = {corr_val:.3f}', \n",
    "            transform=ax.transAxes, fontsize=12, \n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection {#outliers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IQR method\n",
    "def detect_outliers_iqr(df, features):\n",
    "    \"\"\"\n",
    "    Detect outliers using the Interquartile Range (IQR) method.\n",
    "    \"\"\"\n",
    "    outlier_indices = set()\n",
    "    outlier_summary = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        Q1 = df[feature].quantile(0.25)\n",
    "        Q3 = df[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        feature_outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)].index\n",
    "        outlier_indices.update(feature_outliers)\n",
    "        \n",
    "        outlier_summary[feature] = {\n",
    "            'count': len(feature_outliers),\n",
    "            'percentage': (len(feature_outliers) / len(df)) * 100,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound\n",
    "        }\n",
    "    \n",
    "    return list(outlier_indices), outlier_summary\n",
    "\n",
    "# Detect outliers\n",
    "outlier_features = ['mpg', 'horsepower', 'price', 'weight', 'engine_size']\n",
    "outlier_indices, outlier_summary = detect_outliers_iqr(df, outlier_features)\n",
    "\n",
    "print(\"🎯 Outlier Detection Results (IQR Method):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total outlier observations: {len(outlier_indices)} ({(len(outlier_indices)/len(df)*100):.1f}%)\")\n",
    "print("\\nOutliers by feature:")

for feature, stats in outlier_summary.items():
    print(f"• {feature}: {stats['count']} ({stats['percentage']:.1f}%)")
    print(f"  Range: [{stats['lower_bound']:.1f}, {stats['upper_bound']:.1f}]")
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('📦 Outlier Detection - Box Plots', fontsize=16, fontweight='bold')\n",
    "\n",
    "outlier_features_viz = ['mpg', 'horsepower', 'price', 'weight', 'engine_size', 'mileage']\n",
    "\n",
    "for i, feature in enumerate(outlier_features_viz):\n",
    "    row, col = i // 3, i % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Create box plot\n",
    "    box_plot = ax.boxplot(df[feature], patch_artist=True)\n",
    "    box_plot['boxes'][0].set_facecolor('lightblue')\n",
    "    box_plot['boxes'][0].set_alpha(0.7)\n",
    "    \n",
    "    ax.set_title(f'{feature.replace(\"_\", \" \").title()} Outliers')\n",
    "    ax.set_ylabel(feature)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    q1, median, q3 = df[feature].quantile([0.25, 0.5, 0.75])\n",
    "    ax.text(0.02, 0.98, f'Q1: {q1:.1f}\\nMedian: {median:.1f}\\nQ3: {q3:.1f}', \n",
    "            transform=ax.transAxes, fontsize=10, \n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "            verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show some outlier examples\n",
    "if len(outlier_indices) > 0:\n",
    "    print(\"\\n📋 Sample Outlier Vehicles:\")\n",
    "    print(\"=\" * 60)\n",
    "    sample_outliers = df.loc[outlier_indices[:5]]\n",
    "    display_cols = ['vehicle_id', 'brand', 'vehicle_type', 'mpg', 'horsepower', 'price', 'weight']\n",
    "    print(sample_outliers[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering Opportunities {#feature-engineering}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived features that might be useful for clustering\n",
    "print(\"🔧 Feature Engineering Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a copy for feature engineering\n",
    "df_engineered = df.copy()\n",
    "\n",
    "# 1. Power-to-weight ratio\n",
    "df_engineered['power_to_weight'] = df_engineered['horsepower'] / (df_engineered['weight'] / 1000)\n",
    "\n",
    "# 2. Price per horsepower\n",
    "df_engineered['price_per_hp'] = df_engineered['price'] / df_engineered['horsepower']\n",
    "\n",
    "# 3. Fuel efficiency category\n",
    "df_engineered['mpg_category'] = pd.cut(df_engineered['mpg'], \n",
    "                                      bins=[0, 20, 30, 50, float('inf')],\n",
    "                                      labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# 4. Vehicle age\n",
    "current_year = 2024\n",
    "df_engineered['age'] = current_year - df_engineered['year']\n",
    "\n",
    "# 5. Performance category (based on horsepower)\n",
    "df_engineered['performance_tier'] = pd.cut(df_engineered['horsepower'],\n",
    "                                          bins=[0, 150, 250, 350, float('inf')],\n",
    "                                          labels=['Economy', 'Standard', 'Performance', 'High Performance'])\n",
    "\n",
    "# 6. Price segment\n",
    "df_engineered['price_segment'] = pd.cut(df_engineered['price'],\n",
    "                                       bins=[0, 25000, 40000, 60000, float('inf')],\n",
    "                                       labels=['Budget', 'Mid-range', 'Premium', 'Luxury'])\n",
    "\n",
    "# 7. Efficiency score (combination of mpg and weight)\n",
    "# Normalize mpg and inverse weight for scoring\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "mpg_scaled = scaler.fit_transform(df_engineered[['mpg']])[:, 0]\n",
    "weight_scaled = 1 - scaler.fit_transform(df_engineered[['weight']])[:, 0]  # Inverse weight\n",
    "df_engineered['efficiency_score'] = (mpg_scaled + weight_scaled) / 2\n",
    "\n",
    "print(\"✅ Created engineered features:\")\n",
    "engineered_features = ['power_to_weight', 'price_per_hp', 'mpg_category', 'age', \n",
    "                      'performance_tier', 'price_segment', 'efficiency_score']\n",
    "\n",
    "for feature in engineered_features:\n",
    "    if df_engineered[feature].dtype in ['int64', 'float64']:\n",
    "        print(f\"• {feature}: {df_engineered[feature].dtype} (mean: {df_engineered[feature].mean():.2f})\")\n",
    "    else:\n",
    "        unique_count = df_engineered[feature].nunique()\n",
    "        print(f\"• {feature}: categorical ({unique_count} categories)\")\n",
    "\n",
    "# Display sample of engineered features\n",
    "print(\"\\n📊 Sample of Engineered Features:\")\n",
    "sample_cols = ['vehicle_id', 'power_to_weight', 'price_per_hp', 'age', 'efficiency_score']\n",
    "df_engineered[sample_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationships between original and engineered features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('🔧 Engineered Features Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Power-to-weight vs performance\n",
    "sns.boxplot(data=df_engineered, x='performance_tier', y='power_to_weight', ax=axes[0,0])\n",
    "axes[0,0].set_title('Power-to-Weight by Performance Tier')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Efficiency score distribution by fuel type\n",
    "sns.boxplot(data=df_engineered, x='fuel_type', y='efficiency_score', ax=axes[0,1])\n",
    "axes[0,1].set_title('Efficiency Score by Fuel Type')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Price per HP vs price segment\n",
    "sns.boxplot(data=df_engineered, x='price_segment', y='price_per_hp', ax=axes[1,0])\n",
    "axes[1,0].set_title('Price per HP by Price Segment')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Age vs category\n",
    "sns.boxplot(data=df_engineered, x='category', y='age', ax=axes[1,1])\n",
    "axes[1,1].set_title('Vehicle Age by Category')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Preprocessing for Clustering {#preprocessing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering analysis\n",
    "print(\"🔄 Data Preprocessing for Clustering:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Select features for clustering\n",
    "clustering_features = [\n",
    "    'mpg', 'horsepower', 'price', 'weight', 'engine_size',\n",
    "    'power_to_weight', 'price_per_hp', 'efficiency_score', 'age'\n",
    "]\n",
    "\n",
    "# Create clustering dataset\n",
    "df_clustering = df_engineered[clustering_features].copy()\n",
    "\n",
    "print(f\"✅ Selected {len(clustering_features)} features for clustering:\")\n",
    "for feature in clustering_features:\n",
    "    print(f\"  • {feature}\")\n",
    "\n",
    "# Handle missing values (if any)\n",
    "missing_clustering = df_clustering.isnull().sum()\n",
    "if missing_clustering.any():\n",
    "    print(f\"\\n⚠️ Missing values found:\")\n",
    "    print(missing_clustering[missing_clustering > 0])\n",
    "    \n",
    "    # Fill missing values with median\n",
    "    df_clustering = df_clustering.fillna(df_clustering.median())\n",
    "    print(\"✅ Missing values filled with median\")\n",
    "else:\n",
    "    print(\"\\n✅ No missing values in clustering features\")\n",
    "\n",
    "# Feature scaling comparison\n",
    "print(\"\\n📏 Feature Scaling Analysis:\")\n",
    "print(f\"{'Feature':<20} {'Min':<10} {'Max':<10} {'Mean':<10} {'Std':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for feature in clustering_features:\n",
    "    values = df_clustering[feature]\n",
    "    print(f\"{feature:<20} {values.min():<10.2f} {values.max():<10.2f} {values.mean():<10.2f} {values.std():<10.2f}\")\n",
    "\n",
    "# Apply different scaling methods for comparison\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "scaled_data = {}\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    scaled_data[scaler_name] = pd.DataFrame(\n",
    "        scaler.fit_transform(df_clustering),\n",
    "        columns=clustering_features,\n",
    "        index=df_clustering.index\n",
    "    )\n",
    "\n",
    "print(\"\\n✅ Applied scaling methods: StandardScaler, MinMaxScaler, RobustScaler\")\n",
    "\n",
    "# Visualize scaling effects\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('📊 Feature Scaling Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, (scaler_name, data) in enumerate(scaled_data.items()):\n",
    "    # Create box plot for first 5 features\n",
    "    data_subset = data[clustering_features[:5]]\n",
    "    data_subset.boxplot(ax=axes[i])\n",
    "    axes[i].set_title(f'{scaler_name}')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for next notebooks\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Save original and processed datasets\n",
    "df.to_csv('../data/raw_vehicle_data.csv', index=False)\n",
    "df_engineered.to_csv('../data/engineered_vehicle_data.csv', index=False)\n",
    "df_clustering.to_csv('../data/clustering_features.csv', index=False)\n",
    "\n",
    "# Save scaled versions\n",
    "for scaler_name, data in scaled_data.items():\n",
    "    data.to_csv(f'../data/scaled_data_{scaler_name.lower()}.csv', index=False)\n",
    "\n",
    "print(\"💾 Data saved successfully:\")\n",
    "print(\"  • ../data/raw_vehicle_data.csv\")\n",
    "print(\"  • ../data/engineered_vehicle_data.csv\")\n",
    "print(\"  • ../data/clustering_features.csv\")\n",
    "print(\"  • ../data/scaled_data_*.csv (3 files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Summary and Recommendations\n",
    "\n",
    "### Key Findings:\n",
    "1. **Dataset Quality**: The synthetic dataset contains 1,000 vehicles with no missing values\n",
    "2. **Feature Distributions**: Most numerical features show reasonable distributions with some skewness\n",
    "3. **Correlations**: Strong relationships exist between related features (e.g., horsepower vs price)\n",
    "4. **Outliers**: Some outliers detected, particularly in price and performance metrics\n",
    "5. **Categories**: Natural clustering appears evident in the generated categories\n",
    "\n",
    "### Recommendations for Clustering:\n",
    "1. **Feature Selection**: Use engineered features like power_to_weight and efficiency_score\n",
    "2. **Scaling**: StandardScaler recommended for K-means clustering\n",
    "3. **Outlier Handling**: Consider robust scaling or outlier removal\n",
    "4. **Feature Engineering**: Include derived metrics for better separation\n",
    "\n",
    "### Next Steps:\n",
    "- Proceed to K-means clustering analysis (02_kmeans_clustering.ipynb)\n",
    "- Experiment with different numbers of clusters\n",
    "- Apply dimensionality reduction for visualization\n",
    "- Validate clustering results with domain knowledge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}\n",
    "\n",
    "
